{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "deep-learning-assignment-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtuit/deep-learning-2020-assignments/blob/master/deep-learning-assignment-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdIUHZHpYAu0",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning &mdash; Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNZ7VPLYAu3",
        "colab_type": "text"
      },
      "source": [
        "First assignment for the 2020 Deep Learning course (NWI-IMC058) of the Radboud University.\n",
        "\n",
        "_Gijs van Tulder (g.vantulder@cs.ru.nl) and Twan van Laarhoven (tvanlaarhoven@cs.ru.nl)_\n",
        "\n",
        "_September 2020_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANH1RVswYAu7",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "**Names:** Mick Tuit and Maurice Verbrugge\n",
        "\n",
        "**Group:** Group 1\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHGvV9_SYAu-",
        "colab_type": "text"
      },
      "source": [
        "**Instructions:**\n",
        "* Fill in your names and the name of your group.\n",
        "* Answer the questions and complete the code where necessary.\n",
        "* Re-run the whole notebook before you submit your work.\n",
        "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4I_fr04YAvA",
        "colab_type": "text"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "In this assignment you will\n",
        "1. Experiment with gradient descent optimization;\n",
        "2. Derive and implement gradients for binary cross-entropy loss, the sigmoid function and a linear layer;\n",
        "3. Test your gradient implementations with the finite differences method;\n",
        "4. Use these components to implement and train a simple neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzhdadi5YAvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy.optimize\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(suppress=True, precision=6, linewidth=200)\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLEcCLkeYAvU",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Gradient descent optimization\n",
        "\n",
        "Consider the following function with two parameters and its derivatives:\n",
        "\\begin{align}\n",
        "  f(x, y) &= x^2 + y^2 + x (y + 2) + \\cos(3x) \\\\\n",
        "  \\frac{\\partial f}{\\partial x} &= 2x - 3\\sin(3x) + y + 2 \\\\\n",
        "  \\frac{\\partial f}{\\partial y} &= x + 2y \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9910HidJYAvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x, y):\n",
        "    return x ** 2 + y ** 2 + x * (y + 2) + np.cos(3 * x)\n",
        "def grad_x_f(x, y):\n",
        "    return 2 * x - 3 * np.sin(3 * x) + y + 2\n",
        "def grad_y_f(x, y):\n",
        "    return x + 2 * y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r-5F75dYAvi",
        "colab_type": "text"
      },
      "source": [
        "A plot of the function shows that it has multiple local minima:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3QQN7coYAvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_f_contours():\n",
        "    xx, yy = np.meshgrid(np.linspace(-5, 5), np.linspace(-5, 5))\n",
        "    zz = f(xx, yy)\n",
        "    plt.contourf(xx, yy, zz, 50)\n",
        "    plt.contour(xx, yy, zz, 50, alpha=0.2, colors='black', linestyles='solid')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plot_f_contours()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ONyjhGYAvt",
        "colab_type": "text"
      },
      "source": [
        "### Implement gradient descent\n",
        "\n",
        "We would like to find the minimum of this function using gradient descent.\n",
        "\n",
        "**Implement the gradient descent updates for $x$ and $y$ in the function below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq71NtXbYAvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_f(x, y, step_size, steps):\n",
        "    # keep track of the parameters we tried so far\n",
        "    x_hist, y_hist = [x], [y]\n",
        "\n",
        "    # run gradient descent for the number of steps\n",
        "    for step in range(steps):\n",
        "        # compute the gradients at the current point\n",
        "        dx = grad_x_f(x, y)\n",
        "        dy = grad_y_f(x, y)\n",
        "\n",
        "        # apply the gradient descent updates to x and y\n",
        "        x = x  # TODO: compute the update\n",
        "        y = y  # TODO: compute the update\n",
        "\n",
        "        # store the new parameters\n",
        "        x_hist.append(x)\n",
        "        y_hist.append(y)\n",
        "\n",
        "    return x, y, f(x, y), x_hist, y_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CopGkfeyYAv3",
        "colab_type": "text"
      },
      "source": [
        "### Tune the parameters\n",
        "\n",
        "We will now try if our optimization method works.\n",
        "\n",
        "Use this helper function to plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAPsupoGYAv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function that plots the results of the gradient descent optimization\n",
        "def plot_gradient_descent_results(x, y, val, x_hist, y_hist):\n",
        "    # plot the path on the contour plot\n",
        "    plt.figure(figsize=(20, 7))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plot_f_contours()\n",
        "    plt.plot(x_hist, y_hist, '.-')\n",
        "    \n",
        "    # plot the learning curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(f(np.array(x_hist), np.array(y_hist)), '.r-')\n",
        "    plt.title('Minimum value: %f' % f(x_hist[-1], y_hist[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OLROQ7iYAwC",
        "colab_type": "text"
      },
      "source": [
        "**Run the gradient descent optimization with the following initial settings:**\n",
        "\n",
        "``x=3, y=2, step_size=0.1, steps=10``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCzWewafYAwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = optimize_f(x=3, y=2, step_size=0.1, steps=10)\n",
        "plot_gradient_descent_results(*results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwxGozUWYAwK",
        "colab_type": "text"
      },
      "source": [
        "**Does it find the minimum of the function? What happens?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YadRMjloYAwM",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJWDsnJTYAwN",
        "colab_type": "text"
      },
      "source": [
        "**Try a few different values for the `step_size` and the number of `steps` to get closes to the optimal solution:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q87BSzfMYAwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: tune the parameters to find a better optimum\n",
        "results = optimize_f(x=3, y=2, step_size=0.1, steps=10)\n",
        "plot_gradient_descent_results(*results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQo22EE3YAwX",
        "colab_type": "text"
      },
      "source": [
        "**Were you able to find a step size that reached the global optimum? If not, why not?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGUSQy0FYAwZ",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBzG6lpHYAwa",
        "colab_type": "text"
      },
      "source": [
        "### Implement a decreasing step size\n",
        "\n",
        "You might get better results if you use a step size that is large at the beginning, but slowly decreases during the optimization.\n",
        "\n",
        "Try the following scheme to compute the step size $\\eta_t$ in step $t$, given a decay parameter $d$:\n",
        "\\begin{align}\n",
        "  \\eta_t = \\eta_0 d^t\n",
        "\\end{align}\n",
        "\n",
        "**Update your optimization function to use this step size schedule:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBQvQ9c3YAwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_f(x, y, step_size, steps, decay=1.0):\n",
        "    # keep track of the parameters we tried so far\n",
        "    x_hist, y_hist = [x], [y]\n",
        "\n",
        "    # run gradient descent for the number of steps\n",
        "    for step in range(steps):\n",
        "        # compute the gradients at this point\n",
        "        dx = grad_x_f(x, y)\n",
        "        dy = grad_y_f(x, y)\n",
        "\n",
        "        # apply the gradient descent updates to x and y\n",
        "        x = x  # TODO: compute the update including step size decay\n",
        "        y = y  # TODO: compute the update including step size decay\n",
        "\n",
        "        # store the new parameters\n",
        "        x_hist.append(x)\n",
        "        y_hist.append(y)\n",
        "\n",
        "    return x, y, f(x, y), x_hist, y_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIvNDJkdYAwi",
        "colab_type": "text"
      },
      "source": [
        "**Tune the `step_sizes`, `steps` and `decay` parameters to get closer to the global minimum:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOIVAYHjYAwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: tune the parameters to find the local optimum\n",
        "results = optimize_f(x=3, y=2, step_size=0.1, steps=10, decay=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmB0ou_sYAwr",
        "colab_type": "text"
      },
      "source": [
        "We will now look at some more complex functions that we can try to optimize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc04quQDYAwv",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Neural network components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q79-tk_dYAww",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, we will implement a simple neural network from scratch. We need four components:\n",
        "1. A sigmoid activation function,\n",
        "2. A ReLU activation function,\n",
        "3. A binary cross-entropy loss function,\n",
        "4. A linear layer.\n",
        "\n",
        "For each component, we will implement the forward pass, the backward pass, and the gradient descent update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3OoRBAjYAwx",
        "colab_type": "text"
      },
      "source": [
        "### Sigmoid non-linearity\n",
        "\n",
        "The sigmoid function is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "\\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\n",
        "\\end{align}\n",
        "\n",
        "![Sigmoid](attachment:sigmoid.png)\n",
        "\n",
        "**Give the derivative of the sigmoid function:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l6mATphYAw6",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\sigma(x)}{\\partial x} &=\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcj9aKG9YAw7",
        "colab_type": "text"
      },
      "source": [
        "**Implement the sigmoid and its gradient in the functions `sigmoid(x)` and `sigmoid_grad(x)`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF14FD6xYAw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    # TODO: implement the sigmoid function\n",
        "    raise NotImplementedError\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    # TODO: implement the gradient of the sigmoid function\n",
        "    raise NotImplementedError\n",
        "\n",
        "# try with a random input\n",
        "x = np.random.uniform(-10, 10, size=5)\n",
        "print('x:', x)\n",
        "print('sigmoid(x):', sigmoid(x))\n",
        "print('sigmoid_grad(x):', sigmoid_grad(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7MwKGqYAxD",
        "colab_type": "text"
      },
      "source": [
        "To check that the gradient implementation is correct, we can compute the numerical derivative using the finite difference method. From [Chapter 11.5 of the Deep Learning book](http://www.deeplearningbook.org/contents/guidelines.html):\n",
        "\n",
        "> Because\n",
        "  \\begin{align}\n",
        "    f'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x + \\epsilon) - f(x)}{ \\epsilon},\n",
        "  \\end{align}\n",
        "  we can approximate the derivative by using a small, finite $\\epsilon$:\n",
        "  \\begin{align}\n",
        "    f'(x) \\approx \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}.\n",
        "  \\end{align}\n",
        "  We can improve the accuracy of the approximation by using the centered difference:\n",
        "  \\begin{align}\n",
        "    f'(x) \\approx \\frac{f(x + \\frac{1}{2} \\epsilon) - f(x - \\frac{1}{2} \\epsilon)}{\\epsilon}.\n",
        "  \\end{align}\n",
        "  The perturbation size $\\epsilon$ must be large enough to ensure that the perturbation is not rounded down too much by ﬁnite-precision numerical computations.\n",
        "\n",
        "**Use the central difference method to check your implementation of the sigmoid gradient. Compute the numerical gradient and check that it is close to the symbolic gradient computed by your implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7cn0SmNYAxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start with some random inputs\n",
        "x = np.random.uniform(-2, 2, size=5)\n",
        "\n",
        "# compute the symbolic gradient\n",
        "print('Symbolic', sigmoid_grad(x))\n",
        "\n",
        "# TODO: compute the numerical gradient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdFZ-0jKYAxd",
        "colab_type": "text"
      },
      "source": [
        "**Is the gradient computed with finite differences exactly the same as the analytic answer? Why (not)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO8bTBB8YAxe",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2CbtjjAYAxf",
        "colab_type": "text"
      },
      "source": [
        "**If there is a visible difference between the two gradients, please try to make this a small as possible before you continue.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwEV00tLYAxg",
        "colab_type": "text"
      },
      "source": [
        "## Rectified linear units (ReLU)\n",
        "\n",
        "The rectified linear unit is defined as:\n",
        "\\begin{align}\n",
        "  f(x) = \\max(0, x)\n",
        "\\end{align}\n",
        "\n",
        "![relu.png](attachment:relu.png)\n",
        "\n",
        "**Give the derivative of the ReLU function:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTAyG3iCYAxi",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here.\n",
        "\n",
        "\\begin{align}\n",
        "  \\frac{\\partial f(x)}{\\partial x} &=\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjryfjR9YAxj",
        "colab_type": "text"
      },
      "source": [
        "**Implement the ReLU function and its gradient in the functions `relu(x)` and `relu_grad(x)`. Use the finite differences method to check that the gradient is correct:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s2h1LdqYAxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "    # TODO: implement the relu function\n",
        "    raise NotImplementedError\n",
        "\n",
        "def relu_grad(x):\n",
        "    # TODO: implement the gradient of the relu function\n",
        "    raise NotImplementedError\n",
        "\n",
        "# try with a random input\n",
        "x = np.random.uniform(-10, 10, size=5)\n",
        "print('x:', x)\n",
        "print('relu(x):', relu(x))\n",
        "print('relu_grad(x):', relu_grad(x))\n",
        "print()\n",
        "\n",
        "# TODO: compute and compare the symbolic and numerical gradients\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7mtMrRMYAxq",
        "colab_type": "text"
      },
      "source": [
        "### Comparing sigmoid and ReLU\n",
        "\n",
        "The sigmoid and ReLU activation functions have slightly different characteristics.\n",
        "\n",
        "**Run the code below to plot the sigmoid and ReLU activation functions and their gradients:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eAkkqgQYAxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
        "plt.xlabel('x')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(x, relu(x), label='ReLU')\n",
        "plt.xlabel('x')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(x, sigmoid_grad(x), label='Sigmoid gradient')\n",
        "plt.xlabel('x')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(x, relu_grad(x), label='ReLU gradient')\n",
        "plt.xlabel('x')\n",
        "plt.legend(loc='upper left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjr31ieiYAxw",
        "colab_type": "text"
      },
      "source": [
        "**Which activation function would you recommend for a network that outputs probabilities, i.e., outputs $\\in (0, 1)$? Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcm5Hs5kYAxx",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj1JUHOVYAxy",
        "colab_type": "text"
      },
      "source": [
        "**Compare the gradients for sigmoid and ReLU. What are the advantages and disadvantages of each activation function?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmlCIRflYAxz",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzzi9WThYAx1",
        "colab_type": "text"
      },
      "source": [
        "### Binary cross-entropy loss\n",
        "\n",
        "We will use the binary cross-entropy loss to train our network. This loss function is useful for binary classification.\n",
        "\n",
        "The binary cross-entropy (BCE) is a function of the ground truth label $y \\in \\{0, 1\\}$ and the predicted label $\\hat{y} \\in (0, 1)$:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathcal{L} &= -(y \\log{\\hat{y}} + (1-y) \\log(1-\\hat{y})) \\\\\n",
        "\\end{align}\n",
        "\n",
        "To minimize the BCE loss with gradient descent, we need to compute the gradient with respect to the prediction $\\hat{y}$.\n",
        "\n",
        "**Derive the gradient for the BCE loss:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwOEIeLtYAx2",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here.\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &=\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY9ieKauYAx4",
        "colab_type": "text"
      },
      "source": [
        "**Implement `bce_loss(y, y_hat)` and `bce_loss_grad(y, y_hat)` and use the finite differences method to check that the gradient is correct:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhdqS21bYAx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bce_loss(y, y_hat):\n",
        "    # TODO: implement the BCE loss\n",
        "    raise NotImplementedError\n",
        "\n",
        "def bce_loss_grad(y, y_hat):\n",
        "    # TODO: implement the gradient of the BCE loss\n",
        "    raise NotImplementedError\n",
        "\n",
        "# try with some random inputs\n",
        "y = np.random.randint(2, size=5)\n",
        "y_hat = np.random.uniform(0, 1, size=5)\n",
        "print('y:', y)\n",
        "print('y_hat:', y_hat)\n",
        "print('bceloss(y, y_hat):', bce_loss(y, y_hat))\n",
        "print()\n",
        "\n",
        "# TODO: compute and compare the symbolic and numerical gradients\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EnFOsx3YAx9",
        "colab_type": "text"
      },
      "source": [
        "### Linear layer\n",
        "\n",
        "Finally, we need to compute the gradients for the linear layer in our network.\n",
        "\n",
        "Define a linear model $\\mathbf{y} = \\mathbf{x} \\mathbf{W} + \\mathbf{b}$, where\n",
        "* $\\mathbf{x}$ is an input vector of shape $N$,\n",
        "* $\\mathbf{W}$ is a weight matrix of shape $N \\times M$,\n",
        "* $\\mathbf{b}$ is a bias vector of shape $M$,\n",
        "* $\\mathbf{y}$ is the output vector of shape $M$.\n",
        "\n",
        "**Derive the gradients for $\\mathbf{y}$ with respect to the input $\\mathbf{x}$ and the parameters $\\mathbf{W}$ and $\\mathbf{b}$:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBb0XA35YAx_",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here.\n",
        "\n",
        "\\begin{align}\n",
        "  \\nabla_\\mathbf{x} \\mathbf{y} &= \\\\\n",
        "  \\nabla_\\mathbf{W} \\mathbf{y} &= \\\\\n",
        "  \\nabla_\\mathbf{b} \\mathbf{y} &= \\\\\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHGuEiATYAx_",
        "colab_type": "text"
      },
      "source": [
        "**Given the gradient $\\nabla_\\mathbf{y} \\mathcal{L}$ for the loss w.r.t. $\\mathbf{y}$, use the chain rule to derive the gradients for the loss w.r.t. $\\mathbf{x}$, $\\mathbf{W}$ and $\\mathbf{b}$:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VMGxGnYAyC",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here.\n",
        "\n",
        "\\begin{align}\n",
        "  \\nabla_\\mathbf{x} \\mathcal{L} &= \\\\\n",
        "  \\nabla_\\mathbf{W} \\mathcal{L} &= \\\\\n",
        "  \\nabla_\\mathbf{b} \\mathcal{L} &= \\\\\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK-ZtgJkYAyE",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Implement a one-layer model\n",
        "\n",
        "We can now implement a simple one-layer model with a sigmoid activation:\n",
        "\n",
        "1. Given an input vector $\\mathbf{x}$, weight vector $\\mathbf{w}$ and bias $b$, compute the output $\\hat{y}$:\n",
        "\n",
        "\\begin{align}\n",
        "h = \\mathbf{x}^T \\mathbf{w} + b \\\\\n",
        "\\hat{y} = \\sigma(h) \\\\\n",
        "\\end{align}\n",
        "\n",
        "2. Compute the BCE loss comparing the prediction $\\hat{y}$ with the ground-truth label $y$.\n",
        "\n",
        "3. Compute the gradient for the BCE loss and back-propagate this to get the gradient of $\\mathcal{L}$ w.r.t. $\\mathbf{x}$:\n",
        "\n",
        "\\begin{align}\n",
        "  \\nabla_\\mathbf{x} \\mathcal{L} =\n",
        "\\end{align}\n",
        "\n",
        "**Complete the implementation below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvFm3JZ8YAyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize parameters\n",
        "w = np.random.uniform(size=5)\n",
        "b = np.random.rand()\n",
        "\n",
        "# implement the model\n",
        "def fn(x, y):\n",
        "    # TODO: forward: compute h, y_hat, loss\n",
        "    h = 0\n",
        "    y_hat = 0\n",
        "    loss = 0\n",
        "    \n",
        "    # TODO: backward: compute grad_y_hat, grad_h, grad_x\n",
        "    grad_y_hat = 0\n",
        "    grad_h = 0\n",
        "    grad_x = 0\n",
        "    \n",
        "    return loss, grad_x\n",
        "\n",
        "# test with a random input\n",
        "x = np.random.uniform(size=5)\n",
        "y = 1\n",
        "\n",
        "loss, grad_x = fn(x, y)\n",
        "print(\"Loss\", loss)\n",
        "print(\"Gradient\", grad_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnVUerHgYAyJ",
        "colab_type": "text"
      },
      "source": [
        "**Use the finite-difference method to check the gradient $\\nabla_\\mathbf{x} \\mathcal{L}$:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHrxCGdNYAyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start with some random inputs\n",
        "x = np.random.uniform(size=5)\n",
        "y = 1\n",
        "\n",
        "# set epsilon to a small value\n",
        "eps = 0.00001\n",
        "\n",
        "numerical_grad = np.zeros(x.shape)\n",
        "# compute the gradient for each element of x separately\n",
        "for i in range(len(x)):\n",
        "    # compute inputs at -eps/2 and +eps/2\n",
        "    x_a, x_b = x.copy(), x.copy()\n",
        "    x_a[i] += eps / 2\n",
        "    x_b[i] -= eps / 2\n",
        "\n",
        "    # compute the gradient for this element\n",
        "    loss_a, _ = fn(x_a, y)\n",
        "    loss_b, _ = fn(x_b, y)\n",
        "    numerical_grad[i] = (loss_a - loss_b) / eps\n",
        "\n",
        "# compute the symbolic gradient\n",
        "loss, symbolic_grad = fn(x, y)\n",
        "    \n",
        "print(\"Symbolic gradient\")\n",
        "print(symbolic_grad)\n",
        "print(\"Numerical gradient\")\n",
        "print(numerical_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb4rOTPkYAyP",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Implement a linear layer and the sigmoid and ReLU activation functions\n",
        "\n",
        "We will now construct a simple neural network. We need to implement the folowing objects:\n",
        "* `Linear`: a layer that computes `y = x*W + b`.\n",
        "* `Sigmoid`: a layer that computes `y = sigmoid(x)`.\n",
        "* `ReLU`: a layer that computes `y = relu(x)`.\n",
        "\n",
        "For each layer class, we need to implement the following methods:\n",
        "* `forward`: The forward pass that computes the output `y` given `x`.\n",
        "* `backward`: The backward pass that receives the gradient for `y` and computes the gradients for the input `x` and the parameters of the layer.\n",
        "* `step`: The update step that applies the gradient updates to the weights, based on the gradient computed by `backward`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiCowVncYAyQ",
        "colab_type": "text"
      },
      "source": [
        "**Implement a class `Linear` that computes `y = x*W + b`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO19pczGYAyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes y = x * w + b.\n",
        "class Linear:\n",
        "    def __init__(self, n_in, n_out):\n",
        "        # initialize the weights randomly,\n",
        "        # using the Xavier initialization rule for scale\n",
        "        a = np.sqrt(6 / (n_in * n_out))\n",
        "        self.W = np.random.uniform(-a, a, size=(n_in, n_out))\n",
        "        self.b = np.zeros((n_out,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: compute the forward pass\n",
        "        y = 0  # TODO\n",
        "        return y\n",
        "\n",
        "    def backward(self, x, dy):\n",
        "        # TODO: compute the backward pass,\n",
        "        # given dy, compute the gradients for x, W and b\n",
        "        dx = 0       # TODO\n",
        "        self.dW = 0  # TODO\n",
        "        self.db = 0  # TODO\n",
        "        return dx\n",
        "    \n",
        "    def step(self, step):\n",
        "        # TODO: apply a gradient descent update step\n",
        "        self.W = self.W  # TODO\n",
        "        self.b = self.b  # TODO\n",
        "        \n",
        "    def __str__(self):\n",
        "        return 'Linear %dx%d' % self.W.shape\n",
        "\n",
        "\n",
        "# Try the new class with some random values.\n",
        "# Debugging tip: always choose a unique length for each\n",
        "# dimension, so you'll get an error if you mix them up.\n",
        "x = np.random.uniform(size=(3, 5))\n",
        "\n",
        "layer = Linear(5, 7)\n",
        "y = layer.forward(x)\n",
        "dx = layer.backward(x, np.ones_like(y))\n",
        "print('y:', y)\n",
        "print('dx:', dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtiIvmasYAyX",
        "colab_type": "text"
      },
      "source": [
        "**Implement a class `Sigmoid` that computes `y = 1 / (1 + exp(-x))`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRkQOZMnYAyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes y = 1 / (1 + exp(-x)).\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        # TODO: compute the forward pass\n",
        "        raise NotImplementedError  # TODO\n",
        "\n",
        "    def backward(self, x, dy):\n",
        "        # TODO: compute the backward pass,\n",
        "        # return the gradient for x given dy\n",
        "        raise NotImplementedError  # TODO\n",
        "    \n",
        "    def step(self, step_size):\n",
        "        raise NotImplementedError  # TODO\n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'Sigmoid'\n",
        "\n",
        "\n",
        "# try the new class with some random values\n",
        "x = np.random.uniform(size=(3, 5))\n",
        "\n",
        "layer = Sigmoid()\n",
        "y = layer.forward(x)\n",
        "dx = layer.backward(x, np.ones_like(y))\n",
        "print('y:', y)\n",
        "print('dx:', dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeyZKmFHYAyd",
        "colab_type": "text"
      },
      "source": [
        "**Implement a class `ReLU` that computes `y = max(0, x)`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY-NCTjtYAye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes y = max(0, x).\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        # TODO: compute the forward pass\n",
        "        raise NotImplementedError  # TODO\n",
        "\n",
        "    def backward(self, x, dy):\n",
        "        # TODO: compute the backward pass,\n",
        "        # return the gradient for x given dy\n",
        "        raise NotImplementedError  # TODO\n",
        "    \n",
        "    def step(self, step_size):\n",
        "        raise NotImplementedError  # TODO\n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'ReLU'\n",
        "\n",
        "\n",
        "# try the new class with some random values\n",
        "x = np.random.uniform(-10, 10, size=(3, 5))\n",
        "\n",
        "layer = ReLU()\n",
        "y = layer.forward(x)\n",
        "dx = layer.backward(x, np.ones_like(y))\n",
        "print('y:', y)\n",
        "print('dx:', dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_W87gADYAyj",
        "colab_type": "text"
      },
      "source": [
        "### Verify the gradients (using provided code)\n",
        "\n",
        "The code below will check your implementations using SciPy's finite difference implementation [`check_grad`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.check_grad.html). This is similar to what we did manually before, but automates some of the work.\n",
        "\n",
        "**Run the code and check that the error is not too large.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CPd_Au5YAyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Verify gradient computations for Linear\n",
        "# test for dx\n",
        "layer = Linear(5, 7)\n",
        "def test_fn(x):\n",
        "    x = x.reshape(3, 5)\n",
        "    # multiply the output with a constant to check if\n",
        "    # the gradient uses dy\n",
        "    return 2 * np.sum(layer.forward(x))\n",
        "def test_fn_grad(x):\n",
        "    x = x.reshape(3, 5)\n",
        "    # multiply the incoming dy gradient with a constant\n",
        "    return layer.backward(x, 2 * np.ones((3, 7))).flatten()\n",
        "\n",
        "err = scipy.optimize.check_grad(test_fn, test_fn_grad,\n",
        "                                np.random.uniform(-10, 10, size=3 * 5))\n",
        "print(\"err on dx:\", \"OK\" if np.abs(err) < 1e-5 else \"ERROR\", err)\n",
        "\n",
        "# test for dW\n",
        "x = np.random.uniform(size=(3, 5))\n",
        "layer = Linear(5, 7)\n",
        "def test_fn(w):\n",
        "    layer.W = w.reshape(5, 7)\n",
        "    # multiply the output with a constant to check if\n",
        "    # the gradient uses dy\n",
        "    return 2 * np.sum(layer.forward(x))\n",
        "def test_fn_grad(w):\n",
        "    layer.W = w.reshape(5, 7)\n",
        "    # multiply the incoming dy gradient with a constant\n",
        "    layer.backward(x, 2 * np.ones((3, 7)))\n",
        "    return layer.dW.flatten()\n",
        "\n",
        "err = scipy.optimize.check_grad(test_fn, test_fn_grad,\n",
        "                                np.random.uniform(-10, 10, size=5 * 7))\n",
        "print(\"err on dW:\", \"OK\" if np.abs(err) < 1e-5 else \"ERROR\", err)\n",
        "\n",
        "# test for db\n",
        "x = np.random.uniform(size=(3, 5,))\n",
        "layer = Linear(5, 7)\n",
        "def test_fn(b):\n",
        "    layer.b = b\n",
        "    # multiply the output with a constant to check if\n",
        "    # the gradient uses dy\n",
        "    return 2 * np.sum(layer.forward(x))\n",
        "def test_fn_grad(b):\n",
        "    layer.b = b\n",
        "    # multiply the incoming dy gradient with a constant\n",
        "    layer.backward(x, 2 * np.ones((x.shape[0], 7)))\n",
        "    return layer.db\n",
        "\n",
        "err = scipy.optimize.check_grad(test_fn, test_fn_grad,\n",
        "                                np.random.uniform(-10, 10, size=7))\n",
        "print(\"err on db:\", \"OK\" if np.abs(err) < 1e-5 else \"ERROR\", err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpqMrs4vYAyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Verify gradient computation for Sigmoid\n",
        "# test for dx\n",
        "layer = Sigmoid()\n",
        "def test_fn(x):\n",
        "    # multiply the output with a constant to check if\n",
        "    # the gradient uses dy\n",
        "    return np.sum(2 * layer.forward(x))\n",
        "def test_fn_grad(x):\n",
        "    # multiply the incoming dy gradient with a constant\n",
        "    return layer.backward(x, 2 * np.ones(x.shape))\n",
        "\n",
        "err = scipy.optimize.check_grad(test_fn, test_fn_grad,\n",
        "                                np.random.uniform(-10, 10, size=5))\n",
        "print(\"err on dx:\", \"OK\" if np.abs(err) < 1e-5 else \"ERROR\", err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PKTWutZYAys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Verify gradient computation for ReLU\n",
        "# test for dx\n",
        "layer = ReLU()\n",
        "def test_fn(x):\n",
        "    # multiply the output with a constant to check if\n",
        "    # the gradient uses dy\n",
        "    return 2 * np.sum(layer.forward(x))\n",
        "def test_fn_grad(x):\n",
        "    # multiply the incoming dy gradient with a constant\n",
        "    return layer.backward(x, 2 * np.ones(x.shape))\n",
        "\n",
        "err = scipy.optimize.check_grad(test_fn, test_fn_grad,\n",
        "                                np.random.uniform(1, 10, size=5))\n",
        "print(\"err on dx:\", \"OK\" if np.abs(err) < 1e-5 else \"ERROR\", err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2x6mAA5YAyw",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 Construct a neural network with back-propagation\n",
        "\n",
        "We will use the following container class to implement the network:\n",
        "1. The `forward` pass computes the output of each layer. We store the intermediate inputs for the backward pass.\n",
        "2. The `backward` pass computes the gradients for each layer, in reverse order, by using the original input `x` and the gradient `dy` from the previous layer.\n",
        "3. The `step` function will ask each layer to apply the gradient descent updates to its weights.\n",
        "\n",
        "**Read the code below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUf81acoYAyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute the forward pass for each layer\n",
        "        trace = []\n",
        "        for layer in self.layers:\n",
        "            # compute the forward pass\n",
        "            y = layer.forward(x)\n",
        "            # store the original input for the backward pass\n",
        "            trace.append((layer, x))\n",
        "            x = y\n",
        "        # return the final output and the history trace\n",
        "        return y, trace\n",
        "\n",
        "    def backward(self, trace, dy):\n",
        "        # compute the backward pass for each layer\n",
        "        for layer, x in trace[::-1]:\n",
        "            # compute the backward pass using the original input x\n",
        "            dy = layer.backward(x, dy)\n",
        "\n",
        "    def step(self, learning_rate):\n",
        "        # apply the gradient descent updates of each layer\n",
        "        for layer in self.layers:\n",
        "            layer.step(learning_rate)\n",
        "\n",
        "    def __str__(self):\n",
        "        return '\\n'.join(str(l) for l in self.layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLzET9WoYAy3",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 Training the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dFlVy81YAy4",
        "colab_type": "text"
      },
      "source": [
        "We load a simple dataset with 360 handwritten digits.\n",
        "\n",
        "Each sample has $8 \\times 8$ pixels, arranged as a 1D vector of 64 features.\n",
        "\n",
        "We create a binary classification problem with the label 0 for the digits 0 to 4, and 1 for the digits 5 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXRiYd5wYAy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the first two classes of the digits dataset\n",
        "dataset = sklearn.datasets.load_digits()\n",
        "digits_x, digits_y = dataset['data'], dataset['target']\n",
        "\n",
        "# create a binary classification problem\n",
        "digits_y = (digits_y < 5).astype(float)\n",
        "\n",
        "# plot some of the digits\n",
        "plt.figure(figsize=(10, 2))\n",
        "plt.imshow(np.hstack([digits_x[i].reshape(8, 8) for i in range(10)]), cmap='gray')\n",
        "plt.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.axis('off')\n",
        "\n",
        "# normalize the values to [0, 1]\n",
        "digits_x -= np.mean(digits_x)\n",
        "digits_x /= np.std(digits_x)\n",
        "\n",
        "# print some statistics\n",
        "print('digits_x.shape:', digits_x.shape)\n",
        "print('digits_y.shape:', digits_y.shape)\n",
        "print('min, max values:', np.min(digits_x), np.max(digits_x))\n",
        "print('labels:', np.unique(digits_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLv6UufDYAzA",
        "colab_type": "text"
      },
      "source": [
        "We divide the dataset in a train and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71I7JRmuYAzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a 50%/50% train/test split\n",
        "train_prop = 0.5\n",
        "n_train = int(digits_x.shape[0] * train_prop)\n",
        "\n",
        "# shuffle the images\n",
        "idxs = np.random.permutation(digits_x.shape[0])\n",
        "\n",
        "# take a subset\n",
        "x = {'train': digits_x[idxs[:n_train]],\n",
        "     'test':  digits_x[idxs[n_train:]]}\n",
        "y = {'train': digits_y[idxs[:n_train]],\n",
        "     'test':  digits_y[idxs[n_train:]]}\n",
        "\n",
        "print('Training samples:', x['train'].shape[0])\n",
        "print('Test samples:', x['test'].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTKGugpPYAzG",
        "colab_type": "text"
      },
      "source": [
        "We will now implement a function that trains the network. For each epoch, it loops over all minibatches in the training set and updates the network weights. It will then compute the loss and accuracy for the test samples. Finally, it will plot the learning curves.\n",
        "\n",
        "**Read through the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akKC2vmOYAzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(net, x, y, epochs=25, learning_rate=0.001, mb_size=10):\n",
        "    # initialize the loss and accuracy history\n",
        "    loss_hist = {'train': [], 'test': []}\n",
        "    accuracy_hist = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # initialize the loss and accuracy for this epoch\n",
        "        loss = {'train': 0.0, 'test': 0.0}\n",
        "        accuracy = {'train': 0.0, 'test': 0.0}\n",
        "\n",
        "        # first train on training data, then evaluate on the test data\n",
        "        for phase in ('train', 'test'):\n",
        "            # compute the number of minibatches\n",
        "            steps = x[phase].shape[0] // mb_size\n",
        "\n",
        "            # loop over all minibatches\n",
        "            for step in range(steps):\n",
        "                # get the samples for the current minibatch\n",
        "                x_mb = x[phase][(step * mb_size):((step + 1) * mb_size)]\n",
        "                y_mb = y[phase][(step * mb_size):((step + 1) * mb_size), None]\n",
        "\n",
        "                # compute the forward pass through the network\n",
        "                pred_y, trace = net.forward(x_mb)\n",
        "\n",
        "                # compute the current loss and accuracy\n",
        "                loss[phase] += np.mean(bce_loss(y_mb, pred_y))\n",
        "                accuracy[phase] += np.mean((y_mb > 0.5) == (pred_y > 0.5))\n",
        "\n",
        "                # only update the network in the training phase\n",
        "                if phase == 'train':\n",
        "                    # compute the gradient for the loss\n",
        "                    dy = bce_loss_grad(y_mb, pred_y)\n",
        "\n",
        "                    # backpropagate the gradient through the network\n",
        "                    net.backward(trace, dy)\n",
        "\n",
        "                    # update the weights\n",
        "                    net.step(learning_rate)\n",
        "\n",
        "            # compute the mean loss and accuracy over all minibatches\n",
        "            loss[phase] = loss[phase] / steps\n",
        "            accuracy[phase] = accuracy[phase] / steps\n",
        "\n",
        "            # add statistics to history\n",
        "            loss_hist[phase].append(loss[phase])\n",
        "            accuracy_hist[phase].append(accuracy[phase])\n",
        "\n",
        "        print('Epoch %3d: loss[train]=%7.4f  accuracy[train]=%7.4f  loss[test]=%7.4f  accuracy[test]=%7.4f' %\n",
        "              (epoch, loss['train'], accuracy['train'], loss['test'], accuracy['test']))\n",
        "\n",
        "    # plot the learning curves\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    for phase in loss_hist:\n",
        "        plt.plot(loss_hist[phase], label=phase)\n",
        "    plt.title('BCE loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    for phase in accuracy_hist:\n",
        "        plt.plot(accuracy_hist[phase], label=phase)\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwzQ-xl-YAzK",
        "colab_type": "text"
      },
      "source": [
        "We will define a two-layer network:\n",
        "* A linear layer that maps the 64 features of the input to 32 features.\n",
        "* A ReLU activation function.\n",
        "* A linear layer that maps the 32 features to the 1 output features.\n",
        "* A sigmoid activation function that maps the output to [0, 1].\n",
        "\n",
        "**Train the network and inspect the results. Tune the hyperparameters to get a good result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOzG-O5NYAzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct network\n",
        "net = Net([\n",
        "        Linear(64, 32),\n",
        "        ReLU(),\n",
        "        Linear(32, 1),\n",
        "        Sigmoid()])\n",
        "\n",
        "# TODO: tune the hyperparameters\n",
        "fit(net, x, y,\n",
        "    epochs = 25,\n",
        "    learning_rate = 0,\n",
        "    mb_size = 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcFFb7lJYAzN",
        "colab_type": "text"
      },
      "source": [
        "**Which of the hyperparameters (number of epochs, learning rate, minibatch size) was most important? How did they influence your results?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko4EMkyLYAzO",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueLBlW2UYAzO",
        "colab_type": "text"
      },
      "source": [
        "**Repeat the experiment with a the same network, but remove the ReLU activation in the middle: `[Linear, Linear, Sigmoid]`.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqfd3v1yYAzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Your code here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trDdvbMnYAzT",
        "colab_type": "text"
      },
      "source": [
        "**How does the performance of this network compare with the previous network. Can you explain this result? How does removing the ReLU affect the model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RkypVHIYAzT",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aquP9-JyYAzV",
        "colab_type": "text"
      },
      "source": [
        "**Create a network with one linear layer followed by a sigmoid activation:**\n",
        "\n",
        "`net = Net([Linear(...), Sigmoid()]`\n",
        "\n",
        "**Train this network. Compare the results with the `[Linear, ReLU, Linear, Sigmoid]` and `[Linear, Linear, Sigmoid]` networks you trained before, and explain the results.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMvpoc2tYAzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Your code here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK7k9xhBYAzZ",
        "colab_type": "text"
      },
      "source": [
        "**Discuss your results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JcNA6BrYAzZ",
        "colab_type": "text"
      },
      "source": [
        "TODO: Discuss the results here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edfR4e-9YAza",
        "colab_type": "text"
      },
      "source": [
        "**Try a deeper network (e.g., four linear layers) to see if this can improve the results further.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O3TAFRS5YAzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Your code here.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjsxAcknYAze",
        "colab_type": "text"
      },
      "source": [
        "**Discuss your findings. Were you able to obtain a perfect classification? Explain the learning curves.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg--YgIJYAze",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfQ7MI3xYAzf",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 Final questions\n",
        "\n",
        "You now have some experience training neural networks. Time for a few final questions.\n",
        "\n",
        "**What is the influence of the learning rate? What happens if the learning rate is too low or too high?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc-MPQy9YAzh",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkV7hnDYAzi",
        "colab_type": "text"
      },
      "source": [
        "**What is the role of the minibatch size in SGD? Explain the downsides of a minibatch size that is too small or too high.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TJ94rI1YAzi",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhLvKY_hYAzj",
        "colab_type": "text"
      },
      "source": [
        "**In the linear layer, we initialized the weights $w$ with random weights, but we initialized the bias $b$ with zeros. What would happen if the weights $w$ were initialised as zeros? Why is this not a problem for the bias?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2SClRheYAzl",
        "colab_type": "text"
      },
      "source": [
        "TODO: Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3YFdAAoYAzm",
        "colab_type": "text"
      },
      "source": [
        "## The end\n",
        "\n",
        "Well done! Please double check the instructions at the top before you submit your results."
      ]
    }
  ]
}