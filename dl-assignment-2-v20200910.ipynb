{"cells":[{"cell_type":"markdown","source":"# Deep Learning &mdash; Assignment 2 (update 2020.09.10)","metadata":{"cell_id":"00000-cd004bdf-4c87-4014-969b-364cf360f1ce"}},{"cell_type":"markdown","source":"Second assignment for the 2020 Deep Learning course (NWI-IMC058) of the Radboud University.\n\n_Twan van Laarhoven (tvanlaarhoven@cs.ru.nl) and Gijs van Tulder (g.vantulder@cs.ru.nl)_\n\n_September 2020_","metadata":{"cell_id":"00001-d51b70b6-ca51-471c-a9a5-f1dcb14a6f34"}},{"cell_type":"markdown","source":"-----\n\n**Names:**\n\n**Group:**\n\n-----","metadata":{"cell_id":"00002-ba16ab80-a853-4abe-8cfc-fb0a0a3837c4"}},{"cell_type":"markdown","source":"**Instructions:**\n* Fill in your names and the name of your group.\n* Answer the questions and complete the code where necessary.\n* Re-run the whole notebook before you submit your work.\n* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF.","metadata":{"cell_id":"00003-53f76e98-2762-4af0-ac46-ff63d0636e19"}},{"cell_type":"markdown","source":"## Objectives\n\nIn this assignment you will\n1. Learn how to define and train a neural network with pytorch\n2. Experiment with convolutional neural networks\n3. Investigate the effect of dropout and batch normalization","metadata":{"cell_id":"00004-a0e289c0-427d-4073-a630-2f05d67da7e1"}},{"cell_type":"markdown","source":"## Required software\n\nIf you haven't done so already, you will need to install the following additional libraries:\n* `torch` and `torchvision` for PyTorch,\n* `d2l`, the library that comes with [Dive into deep learning](https://d2l.ai) book,\n* `sounddevice` to play audio,\n* `python_speech_features` to compute MFCC features.\n\nAll libraries can be installed with `pip install`.","metadata":{"cell_id":"00005-84a4a68c-5c23-4cb4-8af1-855562cc8126"}},{"cell_type":"code","metadata":{"cell_id":"00006-0d66152e-4b9c-4246-89c8-e6397a8516fe"},"source":"%matplotlib inline\nimport os\nimport numpy as np\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom scipy.io import wavfile","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Digits dataset\n\nThe d2l book uses a dataset of images as a running example (FashionMNIST). In this assignment we will investigate CNNs in a completely different domain: speech recognition.\n\nThe dataset we use is the free spoken digits dataset, which can be found on https://github.com/Jakobovski/free-spoken-digit-dataset. This dataset consists of the digits 0 to 9, spoken by different speakers. The data comes as .wav files.\n\n**Use `git clone` to download the dataset.**","metadata":{"cell_id":"00007-c6397312-a6b8-4d21-810b-42e656e60691"}},{"cell_type":"markdown","source":"Below is a function to load the data. We pad/truncate each sample to the same length.\nThe raw audio is usually stored in 16 bit integers, with a range -32768 to 32767, where 0 represents no signal. Before using the data, it should be normalized. A common approach is to make sure that the data is between 0 and 1 or between -1 and 1.\n\n**Update the below code to normalize the data to a reasonable range**","metadata":{"cell_id":"00008-80499a8b-211c-4186-863d-c550783904ed"}},{"cell_type":"code","metadata":{"cell_id":"00009-2921c481-c4ac-4a18-8e54-a29b76cc6750"},"source":"samplerate = 8000\ndef load_waveform(file, size = 6000):\n    samplerate, waveform = wavfile.read(file)\n    # Take first 6000 samples from waveform. With a samplerate of 8000 that corresponds to 3/4 second\n    # Pad with 0s if the file is shorter\n    waveform = np.pad(waveform,(0,size))[0:size]\n    # Normalize waveform\n    # TODO: Your code here.\n    return waveform","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code loads all .wav files in a directory, and makes it available in a pytorch dataset.\n\n**Load the data into a variable `data`**","metadata":{"cell_id":"00010-e6bb9c38-53d6-4ffb-b0a8-5494d7098f70"}},{"cell_type":"code","metadata":{"cell_id":"00011-ac0d149f-dbfe-4738-b9a0-7a52c550c4ad"},"source":"class SpokenDigits(torch.utils.data.Dataset):\n    def __init__(self, data_dir):\n        digits_x = []\n        digits_y = []\n        for file in os.listdir(data_dir):\n            if file.endswith(\".wav\"):\n                waveform = load_waveform(os.path.join(data_dir, file))\n                label = int(file[0])\n                digits_x.append(waveform)\n                digits_y.append(label)\n        # convert to torch tensors\n        self.x = torch.from_numpy(np.array(digits_x, dtype=np.float32))\n        self.x = self.x.unsqueeze(1) # One channel\n        self.y = torch.from_numpy(np.array(digits_y))\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n# TODO: Your code here.\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Describe the dataset: how many samples are there, what is their dimensionality? How many classes are there?**","metadata":{"cell_id":"00012-60719ed6-9052-426f-b1c0-5d0ad12a2120"}},{"cell_type":"markdown","source":"TODO: your answer here.","metadata":{"cell_id":"00013-46d97154-59c5-4665-b31a-6d576481fc5f"}},{"cell_type":"markdown","source":"Here is code to play samples from the dataset to give you an idea what it \"looks\" like.","metadata":{"cell_id":"00014-580076a0-cd36-4f81-a1f1-ce0d0adda688"}},{"cell_type":"code","metadata":{"cell_id":"00015-2820d7da-18a3-4cd1-a6b7-7b6437f91d77"},"source":"import sounddevice as sd\ndef play(sample):\n    sd.play(sample[0][0], samplerate)\n    print(sample[1])\nplay(data[0])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00016-c0cb0fef-272b-434e-8d18-b8238136435d"},"source":"train_prop = 2/3\ntrain_count = int(len(data) * train_prop)\ntrain, test = torch.utils.data.random_split(data, [train_count, len(data)-train_count])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code above is code to split the data into a training and test set. It uses 2/3 of the data for training.\n\n**Discuss an advantage and disadvantage of using more of the data for training**","metadata":{"cell_id":"00017-51d18455-a342-4a24-b854-08d374a12135"}},{"cell_type":"markdown","source":"TODO: your answer here.","metadata":{"cell_id":"00018-fa0cd8e3-ae41-4162-831b-22bcf9ce6878"}},{"cell_type":"markdown","source":"Finally, we split the data into batches:","metadata":{"cell_id":"00019-8062ff7c-348a-43e1-a0d6-b9ee2ad2a361"}},{"cell_type":"code","metadata":{"cell_id":"00020-b1afb964-50de-49a6-9371-9dbe27ca25d9"},"source":"data_params = {'batch_size': 32}\ntrain_iter = torch.utils.data.DataLoader(train, **data_params)\ntest_iter  = torch.utils.data.DataLoader(test,  **data_params)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 One dimensional convolutional neural network","metadata":{"cell_id":"00021-0a150155-0bad-43ee-947a-b1e9893e5eda"}},{"cell_type":"markdown","source":"We will now define a network architecture. We will use a combination of convolutional layers and pooling.\nNote that we use 1d convolution and pooling here, instead of the 2d operations used for images.\n\n**Complete the network architecture, look at the d2l book chapters 6 and 7 for examples**","metadata":{"cell_id":"00022-75ffe453-912b-4eb8-b22d-ae44e2420a61"}},{"cell_type":"code","metadata":{"cell_id":"00023-e592eb0e-5114-469b-b1b1-df5f71f347f9"},"source":"net = torch.nn.Sequential(\n    nn.Conv1d(1, 4, kernel_size=5), nn.ReLU(),\n    nn.AvgPool1d(kernel_size=2, stride=2),\n    # TODO: Add three more convolutional layers, ReLU layers and pooling layers;\n    #       doubling the number of channels each time\n    # TODO: Your code here.\n    nn.Flatten(),\n    nn.Linear(11872, 128), nn.ReLU(),\n    nn.Linear(128, 64), nn.ReLU(),\n    nn.Linear(64, 10))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The first fully connected layer has input dimension 11872, where does that number come from?**","metadata":{"cell_id":"00024-56fa150a-2080-40c8-8a60-acf22e1be317"}},{"cell_type":"markdown","source":"TODO: your answer here\n\nHint: think about how (valid) convolutional layers and pooling layers with stride affect the size of the data.","metadata":{"cell_id":"00025-25ce19ad-5196-4fa8-98ca-26e5c7e6acd1"}},{"cell_type":"markdown","source":"**How many parameters are there in the model? I.e. the total number of weights and biases**","metadata":{"cell_id":"00026-a359abdb-1b98-405f-8f03-2e721cb31a73"}},{"cell_type":"code","metadata":{"cell_id":"00027-9afb74a4-1c88-4499-90d0-b417555839a9"},"source":"# TODO: Compute the number of parameters\n# Hint: use net.parameters() and param.nelement()\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Suppose that instead of using convolutions, we had used only fully connected layers. How many parameters would be needed in that case approximately?**","metadata":{"cell_id":"00028-17df227d-532d-4e39-9fd5-5c9d56b7c94e"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00029-a890583f-d32b-4550-b816-99115f615727"}},{"cell_type":"markdown","source":"The FashionMNIST dataset used in the book has 60000 training examples. How large is our training set? How would the difference affect the number of epochs that we need? Compare to chapter 6.6 and 7.1 of the book.\n\n**How many epochs do you think are needed?**","metadata":{"cell_id":"00030-8a3bf46b-6658-4c9a-beb7-c0b93fd55c5d"}},{"cell_type":"code","metadata":{"cell_id":"00031-26b4a6e5-3ccd-46e0-9944-5ba0e6425d20"},"source":"lr, num_epochs = 0.01, 10 # TODO: change\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the code from the d2l book to train the network.\nIn particular, the `train_ch6` function, defined in [chapter 6.6](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html#training). This function is available in the `d2l` library.\nHowever, this function has a bug: it only initializes the weights for 2d convolutional layers, not for 1d convolutional layers.\n\n**Make a copy of the train_ch6 function, and correct the error**","metadata":{"cell_id":"00032-c0f14d81-658f-42e2-aebf-b134046e1231"}},{"cell_type":"code","metadata":{"cell_id":"00033-65661672-8151-4716-bae1-623a2393c365"},"source":"def train(net, train_iter, test_iter, num_epochs, lr, device=d2l.try_gpu()):\n    # TODO: your code here (copied and corrected from train_ch6)\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now train the network.**","metadata":{"cell_id":"00034-c1a9f701-899f-4faa-aad0-6a275b8789a9"}},{"cell_type":"code","metadata":{"cell_id":"00035-57f4337c-9546-446c-af50-b66ce6fd7515"},"source":"train(net, train_iter, test_iter, num_epochs, lr)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Is the training converged?**\n\nIf the training has not converged, maybe you need to change the number of epochs and/or the learning rate.\n\nTODO: Document the changes that you made and their effect.","metadata":{"cell_id":"00036-522a1c6b-d5e7-4b0a-8e53-e7a06a0e1534"}},{"cell_type":"markdown","source":"## 2.3 Questions and evaluation","metadata":{"cell_id":"00037-68dd1e49-c0e0-4603-aedd-28825bb23073"}},{"cell_type":"markdown","source":"**Does the network look like it is overfitting or underfitting?**","metadata":{"cell_id":"00038-40cf4841-81e0-4852-8f83-ccae8a0aee9e"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00039-29b7ac99-f002-421a-8d2b-a0b44a73f3d7"}},{"cell_type":"markdown","source":"**Is what we have here a good classifier? Could it be used in a realistic application?**","metadata":{"cell_id":"00040-44c78055-7a19-4506-9dbb-026b732f5318"}},{"cell_type":"markdown","source":"TODO: discuss your answer","metadata":{"cell_id":"00041-c5ca161d-1eaa-419e-888b-007b8a9730cb"}},{"cell_type":"markdown","source":"**Do you think there is enough training data compared to the dimensions of the data and the number of parameters?**","metadata":{"cell_id":"00042-95c5c262-03b5-4cd1-b495-6a7da3e1ad42"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00043-dd136e61-113a-4e01-974e-1da32182ffe6"}},{"cell_type":"markdown","source":"**How could the classifier be improved?**","metadata":{"cell_id":"00044-e809df6e-aff4-43d6-9273-92655e6686d8"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00045-0e7e1e59-10bc-406f-bd5f-8d3f02f4939d"}},{"cell_type":"markdown","source":"**The free spoken digits datasets has recordings from several different speakers. Is the test set accuracy a good measure of how well the trained network would perform for recognizing the voice of a new speaker? And if not, how could that be tested instead?**","metadata":{"cell_id":"00046-17f02796-2b97-42e1-b8e9-d88bc15ba347"}},{"cell_type":"markdown","source":"TODO: your answer here.","metadata":{"cell_id":"00047-f48319d8-f912-4473-aa4d-791523f5f8a7"}},{"cell_type":"markdown","source":"## 2.4 Variations\n\nOne way in which the training might be improved is with dropout or with batch normalization.\n\n**Make a copy of the network architecture below, and add dropout**\n\nHint: see [chapter 7.1](http://d2l.ai/chapter_convolutional-modern/alexnet.html#architecture) for an example that uses dropout.","metadata":{"cell_id":"00048-9d541aaa-b9db-4620-bdf9-f7e7c4c11d6b"}},{"cell_type":"code","metadata":{"cell_id":"00049-77057138-567c-4b80-970e-0bb79a1e9da6"},"source":"net_dropout = \"TODO: your network here\"\ntrain(net_dropout, train_iter, test_iter, num_epochs, lr)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**How does dropout change the results?**","metadata":{"cell_id":"00050-aba956f5-9ed5-416d-a690-e45bdc5cb305"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00051-746831d3-6a62-492a-8148-8d104a929278"}},{"cell_type":"markdown","source":"**Make a copy of the original network architecture, and add batch normalization to all convolutional and linear layers.**\n\nHint: see [chapter 7.5](http://d2l.ai/chapter_convolutional-modern/batch-norm.html#concise-implementation) for an example.","metadata":{"cell_id":"00052-e1ffbe62-7d29-4217-b639-d01e5e49af93"}},{"cell_type":"code","metadata":{"cell_id":"00053-2091dd1e-7870-4eb3-8b96-3e464a0138af"},"source":"net_batchnorm = \"TODO: your network here\"\ntrain(net_batchnorm, train_iter, test_iter, num_epochs, lr)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**How does batch normalization change the results?**","metadata":{"cell_id":"00054-1e599729-87c7-4c19-a240-3963f4ca72ee"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00055-76051e1d-1c9b-43fa-bdd6-53f750f8b438"}},{"cell_type":"markdown","source":"## 2.5 Bonus: feature extraction\n\nGiven enough training data a deep neural network can learn to extract features from raw data like audio and images. However, in some cases it is still necesary to do manual feature extraction. For speech recognition, a popular class of features are [MFCCs](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).\n\nHere is code to extract these features. You will need to install the `python_speech_features` first.","metadata":{"cell_id":"00056-68e90e8f-ab8b-435e-8955-347027c3d368"}},{"cell_type":"code","metadata":{"cell_id":"00057-ad1270e3-c7f7-4070-b4ea-8420b9367037"},"source":"from python_speech_features import mfcc\n\ndef load_waveform_mfcc(file, size = 6000):\n    samplerate, waveform = wavfile.read(file)\n    waveform = np.pad(waveform,(0,size))[0:size] / 32768\n    return np.transpose(mfcc(waveform, samplerate))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implement a variation of the dataset that uses these features**","metadata":{"cell_id":"00058-e1d24783-b73a-4ac5-9df6-9b08517b6a5b"}},{"cell_type":"code","metadata":{"cell_id":"00059-875039f6-d174-451e-82da-d05a4df96334"},"source":"class SpokenDigitsMFCC(torch.utils.data.Dataset):\n    # TODO: Your code here.\n    pass\n\ndata_mfcc = SpokenDigitsMFCC(data_dir) # TODO: your data directory here\ntrain_count_mfcc = int(len(data_mfcc) * train_prop)\ntrain_mfcc, test_mfcc = torch.utils.data.random_split(data, [train_count_mfcc, len(data_mfcc)-train_count_mfcc])\ntrain_iter_mfcc = torch.utils.data.DataLoader(train_mfcc, **data_params)\ntest_iter_mfcc  = torch.utils.data.DataLoader(test_mfcc,  **data_params)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The MFCC features will have 13 channels instead of 1 (the `unsqueeze` operation is not needed). \n\n**Inspect the shape of the data, and define a new network architecture that accepts data with this shape**","metadata":{"cell_id":"00060-ccdd65c0-ec81-472a-897c-724147cfdfa1"}},{"cell_type":"code","metadata":{"cell_id":"00061-31ac7da8-2e73-43c0-bb82-47e3f7fa3cf2"},"source":"# Your code here.","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the network with the mfcc features.**","metadata":{"cell_id":"00062-c9517468-6244-4e53-812a-6555a0860ec3"}},{"cell_type":"code","metadata":{"cell_id":"00063-6aade5e6-660b-42d7-87f8-5fc7325df8a1"},"source":"# Your code here.","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Is there a neural-network based alternative to mfcc features?**","metadata":{"cell_id":"00064-fa095ed5-ff82-430a-9c28-80a8268a8e14"}},{"cell_type":"markdown","source":"TODO: your answer here","metadata":{"cell_id":"00065-06b46f09-4bbe-46a1-a727-fdfc4612ccaf"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"deepnote_notebook_id":"efd88956-84b5-4a3e-8162-00ba15be3d98","deepnote_execution_queue":[]}}